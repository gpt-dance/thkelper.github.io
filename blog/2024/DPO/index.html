<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="目前在-fsdp-策略配置碰到问题">目前在 FSDP 策略配置碰到问题</h1> <h3 id="将llava_onevision模型合入到verl框架中">将llava_onevision模型合入到verl框架中</h3> <ol> <li><strong>设置 FSDP min_num_params: 100000000</strong></li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">logits</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span> <span class="o">@</span> <span class="n">vocab_weights</span><span class="p">.</span><span class="nf">t</span><span class="p">())</span> <span class="o">/</span> <span class="n">temperature</span>

<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">setStorage</span><span class="p">:</span> <span class="n">sizes</span> <span class="p">[</span><span class="mi">896</span><span class="p">,</span> <span class="mi">152000</span><span class="p">],</span> <span class="n">strides</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">896</span><span class="p">],</span> <span class="n">storage</span> <span class="n">offset</span> <span class="mi">0</span><span class="p">,</span> <span class="ow">and</span> <span class="n">itemsize</span> <span class="mi">4</span> <span class="n">requiring</span> <span class="n">a</span> <span class="n">storage</span> <span class="n">size</span> <span class="n">of</span> <span class="mi">544768000</span> <span class="n">are</span> <span class="n">out</span> <span class="n">of</span> <span class="n">bounds</span> <span class="k">for</span> <span class="n">storage</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">0</span>
</code></pre></div></div> <ol> <li> <h3 id="设置-fsdp-min_num_params-140000000"><strong>设置 FSDP min_num_params: 140000000</strong></h3> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1762</span><span class="p">,</span> <span class="ow">in</span> <span class="n">_call_impl</span>

<span class="err">​</span>    <span class="k">return</span> <span class="nf">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/mnt/largeml-train-gui-agent/yangchangpeng/codes/gui_lab/tiny-mllm/ori_verl/verl/models/transformers/llava_onevision.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">341</span><span class="p">,</span> <span class="ow">in</span> <span class="n">llava_onevision_base_forward</span>

<span class="err">​</span>    <span class="n">kwargs</span><span class="p">[</span><span class="sh">"</span><span class="s">inputs_embeds</span><span class="sh">"</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="sh">"</span><span class="s">attention_mask</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="nf">_get_input_embeds</span><span class="p">(</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/mnt/largeml-train-gui-agent/yangchangpeng/codes/gui_lab/tiny-mllm/ori_verl/verl/models/transformers/llava_onevision.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">296</span><span class="p">,</span> <span class="ow">in</span> <span class="n">_get_input_embeds</span>

<span class="err">​</span>    <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">language_model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">()(</span><span class="n">input_ids</span><span class="p">)</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1751</span><span class="p">,</span> <span class="ow">in</span> <span class="n">_wrapped_call_impl</span>

<span class="err">​</span>    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1762</span><span class="p">,</span> <span class="ow">in</span> <span class="n">_call_impl</span>

<span class="err">​</span>    <span class="k">return</span> <span class="nf">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">190</span><span class="p">,</span> <span class="ow">in</span> <span class="n">forward</span>

<span class="err">​</span>    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span>

  <span class="n">File</span> <span class="sh">"</span><span class="s">/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py</span><span class="sh">"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">2551</span><span class="p">,</span> <span class="ow">in</span> <span class="n">embedding</span>

<span class="err">​</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">sparse</span><span class="p">)</span>

<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">The</span> <span class="n">tensor</span> <span class="n">has</span> <span class="n">a</span> <span class="n">non</span><span class="o">-</span><span class="n">zero</span> <span class="n">number</span> <span class="n">of</span> <span class="n">elements</span><span class="p">,</span> <span class="n">but</span> <span class="n">its</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">allocated</span> <span class="n">yet</span><span class="p">.</span>
</code></pre></div></div> <ol> <li> <h3 id="设置--transformer_layer_cls_to_wrap-"><strong>设置 transformer_layer_cls_to_wrap: [</strong></h3> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">Qwen2DecoderLayer</span><span class="sh">"</span><span class="p">,</span>           <span class="c1"># Qwen2语言模型的transformer层
</span>   
<span class="sh">"</span><span class="s">SiglipEncoderLayer</span><span class="sh">"</span><span class="p">,</span>          <span class="c1"># SigLIP视觉编码器层
</span>   
<span class="sh">"</span><span class="s">SiglipVisionEmbeddings</span><span class="sh">"</span><span class="p">,</span>      <span class="c1"># SigLIP视觉嵌入
</span></code></pre></div> </div> </li> </ol> <p>显卡利用率100%，都停滞，过一段时间ray说worker挂掉任务失败。</p> <ol> <li> <h3 id="按照verl开发者说的you-have-to-ensure-that-the-tied-weights-embedding-is-wrapped-inside-the-same-fsdp-unit">按照Verl开发者说的You have to ensure that the tied weights (embedding) is wrapped inside the same FSDP unit</h3> </li> </ol> <p>从模型结构来看已经没有将embedding和lm_heads包装，显卡不会hang住，但是会报和第二点一样的错误RuntimeError: The tensor has a non-zero number of elements, but its data is not allocated yet.</p> <p>从目前报错信息来看，应该还是FSDP策略出的问题，但不知道到底是哪个细节出的问题导致的。目前发现一个额外可能的错误细节，我在使用的这个llava_onevison模型的配置文件如下，所以embed_tokens和lm_head不是共享权重的，verl中有一行log显示 Some weights of LlavaOnevisionForConditionalGeneration were not initialized from the model checkpoint at /mnt/largeml-model-gui-agent/llava-onevision-qwen2-0.5b-ov-hf and are newly initialized: [‘lm_head.weight’]</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="sh">"</span><span class="s">architectures</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
      <span class="sh">"</span><span class="s">Qwen2ForCausalLM</span><span class="sh">"</span><span class="p">{</span>
    <span class="sh">"</span><span class="s">tie_word_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
  <span class="p">},</span>
  <span class="sh">"</span><span class="s">tie_word_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">FullyShardedDataParallel</span><span class="p">(</span>
  <span class="p">(</span><span class="n">_fsdp_wrapped_module</span><span class="p">):</span> <span class="nc">LlavaOnevisionForConditionalGeneration</span><span class="p">(</span>
    <span class="p">(</span><span class="n">model</span><span class="p">):</span> <span class="nc">LlavaOnevisionModel</span><span class="p">(</span>
      <span class="p">(</span><span class="n">vision_tower</span><span class="p">):</span> <span class="nc">SiglipVisionModel</span><span class="p">(</span>
        <span class="p">(</span><span class="n">vision_model</span><span class="p">):</span> <span class="nc">SiglipVisionTransformer</span><span class="p">(</span>
          <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="nc">SiglipVisionEmbeddings</span><span class="p">(</span>
            <span class="p">(</span><span class="n">patch_embedding</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1152</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="n">valid</span><span class="p">)</span>
            <span class="p">(</span><span class="n">position_embedding</span><span class="p">):</span> <span class="nc">Embedding</span><span class="p">(</span><span class="mi">729</span><span class="p">,</span> <span class="mi">1152</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">encoder</span><span class="p">):</span> <span class="nc">SiglipEncoder</span><span class="p">(</span>
            <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="nc">FullyShardedDataParallel</span><span class="p">(</span>
              <span class="p">(</span><span class="n">_fsdp_wrapped_module</span><span class="p">):</span> <span class="nc">ModuleList</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">25</span><span class="p">):</span> <span class="mi">26</span> <span class="n">x</span> <span class="nc">SiglipEncoderLayer</span><span class="p">(</span>
                  <span class="p">(</span><span class="n">layer_norm1</span><span class="p">):</span> <span class="nc">LayerNorm</span><span class="p">((</span><span class="mi">1152</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">self_attn</span><span class="p">):</span> <span class="nc">SiglipAttention</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">k_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="p">(</span><span class="n">v_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="p">(</span><span class="n">q_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="p">(</span><span class="n">out_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">)</span>
                  <span class="p">(</span><span class="n">layer_norm2</span><span class="p">):</span> <span class="nc">LayerNorm</span><span class="p">((</span><span class="mi">1152</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">mlp</span><span class="p">):</span> <span class="nc">SiglipMLP</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">activation_fn</span><span class="p">):</span> <span class="nc">PytorchGELUTanh</span><span class="p">()</span>
                    <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4304</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="p">(</span><span class="n">fc2</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4304</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">)</span>
                <span class="p">)</span>
              <span class="p">)</span>
            <span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">post_layernorm</span><span class="p">):</span> <span class="nc">LayerNorm</span><span class="p">((</span><span class="mi">1152</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">multi_modal_projector</span><span class="p">):</span> <span class="nc">LlavaOnevisionMultiModalProjector</span><span class="p">(</span>
        <span class="p">(</span><span class="n">linear_1</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1152</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="nc">GELUActivation</span><span class="p">()</span>
        <span class="p">(</span><span class="n">linear_2</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">language_model</span><span class="p">):</span> <span class="nc">FullyShardedDataParallel</span><span class="p">(</span>
        <span class="p">(</span><span class="n">_fsdp_wrapped_module</span><span class="p">):</span> <span class="nc">Qwen2Model</span><span class="p">(</span>
          <span class="p">(</span><span class="n">embed_tokens</span><span class="p">):</span> <span class="nc">Embedding</span><span class="p">(</span><span class="mi">152000</span><span class="p">,</span> <span class="mi">896</span><span class="p">)</span>
          <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">23</span><span class="p">):</span> <span class="mi">24</span> <span class="n">x</span> <span class="nc">FullyShardedDataParallel</span><span class="p">(</span>
              <span class="p">(</span><span class="n">_fsdp_wrapped_module</span><span class="p">):</span> <span class="nc">Qwen2DecoderLayer</span><span class="p">(</span>
                <span class="p">(</span><span class="n">self_attn</span><span class="p">):</span> <span class="nc">Qwen2Attention</span><span class="p">(</span>
                  <span class="p">(</span><span class="n">q_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">k_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">v_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">o_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="p">(</span><span class="n">mlp</span><span class="p">):</span> <span class="nc">Qwen2MLP</span><span class="p">(</span>
                  <span class="p">(</span><span class="n">gate_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4864</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">up_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4864</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">down_proj</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4864</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                  <span class="p">(</span><span class="n">act_fn</span><span class="p">):</span> <span class="nc">SiLU</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="p">(</span><span class="n">input_layernorm</span><span class="p">):</span> <span class="nc">Qwen2RMSNorm</span><span class="p">((</span><span class="mi">896</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span>
                <span class="p">(</span><span class="n">post_attention_layernorm</span><span class="p">):</span> <span class="nc">Qwen2RMSNorm</span><span class="p">((</span><span class="mi">896</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span>
              <span class="p">)</span>
            <span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="nc">Qwen2RMSNorm</span><span class="p">((</span><span class="mi">896</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span>
          <span class="p">(</span><span class="n">rotary_emb</span><span class="p">):</span> <span class="nc">Qwen2RotaryEmbedding</span><span class="p">()</span>
        <span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">lm_head</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">896</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">152000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> </body></html>